{"cells":[{"cell_type":"code","execution_count":null,"id":"423d139c-fe55-40dc-842b-bc2c0d92790c","metadata":{"id":"423d139c-fe55-40dc-842b-bc2c0d92790c"},"outputs":[],"source":["import cv2\n","from deepface import DeepFace\n","import os\n","import time\n","import numpy as np\n","from collections import Counter\n","import tensorflow as tf\n","import csv # Added for CSV logging\n","\n","print(\"Libraries imported successfully.\")\n","\n","# --- GPU Check ---\n","gpus = tf.config.list_physical_devices('GPU')\n","if gpus:\n","    try:\n","        for gpu in gpus:\n","            tf.config.experimental.set_memory_growth(gpu, True)\n","        print(f\"GPU(s) found and configured: {len(gpus)}\")\n","        print(\"DeepFace will now use the GPU for processing.\")\n","    except RuntimeError as e:\n","        print(e)\n","else:\n","    print(\"No GPU found. DeepFace will use the CPU. Processing will be slower.\")\n","    print(\"For faster performance, please ensure you have a compatible NVIDIA GPU, CUDA, cuDNN, and the correct TensorFlow package installed.\")\n","\n","# ==============================================================================\n","# Step 3: Define the core analysis and visualization functions\n","# ==============================================================================\n","def analyze_frame_attributes(frame):\n","    \"\"\"\n","    This function takes a single video frame (as a numpy array), detects faces,\n","    and analyzes age, gender, and emotion for each face.\n","    \"\"\"\n","    try:\n","        results = DeepFace.analyze(\n","            img_path=frame,\n","            actions=['age', 'gender', 'emotion'],\n","            enforce_detection=False,\n","            detector_backend='retinaface'\n","        )\n","        return results\n","    except Exception as e:\n","        return []\n","\n","def draw_results_on_frame(frame, results):\n","    \"\"\"\n","    Draws the analysis results (bounding boxes, text) on the given frame.\n","    \"\"\"\n","    annotated_frame = frame.copy()\n","\n","    for face_data in results:\n","        box = face_data['region']\n","        x, y, w, h = box['x'], box['y'], box['w'], box['h']\n","\n","        gender = face_data['dominant_gender']\n","        age = face_data['age']\n","        emotion = face_data['dominant_emotion']\n","\n","        color = (255, 0, 0) if gender == 'Man' else (203, 192, 255)\n","\n","        cv2.rectangle(annotated_frame, (x, y), (x + w, y + h), color, 2)\n","\n","        text_label = f\"{age}, {gender}, {emotion}\"\n","\n","        (text_width, text_height), _ = cv2.getTextSize(text_label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n","        cv2.rectangle(annotated_frame, (x, y - text_height - 10), (x + text_width + 4, y), color, -1)\n","        cv2.putText(annotated_frame, text_label, (x + 2, y - 5),\n","                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n","\n","    return annotated_frame\n","\n","def create_stats_display(num_people, avg_age, gender_counts, emotion_counts, excitement_index_men, excitement_index_women, window_width=400, window_height=350):\n","    \"\"\"Creates a display image for crowd statistics, now with gender-specific excitement indexes.\"\"\"\n","    display = np.zeros((window_height, window_width, 3), dtype=np.uint8)\n","    display[:] = (40, 40, 40) # Dark gray background\n","\n","    cv2.putText(display, \"Crowd Statistics\", (window_width // 2 - 100, 30),\n","                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n","\n","    cv2.putText(display, f\"People Detected: {num_people}\", (20, 70),\n","                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n","\n","    avg_age_text = f\"Average Age: {avg_age:.1f}\" if num_people > 0 else \"Average Age: N/A\"\n","    cv2.putText(display, avg_age_text, (20, 105),\n","                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)\n","\n","    men_count = gender_counts.get('Man', 0)\n","    women_count = gender_counts.get('Woman', 0)\n","    cv2.putText(display, f\"Men: {men_count}\", (20, 140),\n","                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n","    cv2.putText(display, f\"Women: {women_count}\", (180, 140),\n","                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (203, 192, 255), 2)\n","\n","    cv2.putText(display, \"Emotion Breakdown:\", (20, 175),\n","                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n","\n","    top_emotions = emotion_counts.most_common(2)\n","    y_offset = 205\n","    for emotion, count in top_emotions:\n","        emotion_text = f\"- {emotion}: {count}\"\n","        cv2.putText(display, emotion_text, (25, y_offset),\n","                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 2)\n","        y_offset += 25\n","\n","    # --- MODIFIED: Excitement Index Display for both genders ---\n","    y_offset += 15\n","    cv2.putText(display, f\"Men Excitement: {excitement_index_men:.2f}\", (20, y_offset),\n","                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 150, 150), 2) # Light Blue\n","    y_offset += 30\n","    cv2.putText(display, f\"Women Excitement: {excitement_index_women:.2f}\", (20, y_offset),\n","                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (203, 192, 255), 2) # Pink\n","\n","    return display\n","\n","# ==============================================================================\n","# Step 4: Define video paths and run the analysis pipeline\n","# ==============================================================================\n","video_path = r\"C:\\vid_path\"\n","output_video_path = r\"output_video_with_stats_retinaface.mp4\"\n","# --- ADDED: Define output CSV file path ---\n","output_csv_path = r\"crowd_analysis_log.csv\"\n","\n","if not os.path.exists(video_path):\n","    print(f\"ERROR: The video file was not found at the specified path: {video_path}\")\n","else:\n","    print(f\"Found video, starting analysis on: {video_path}\")\n","    print(f\"Statistical logs will be saved to: {output_csv_path}\")\n","\n","    cap = cv2.VideoCapture(video_path)\n","\n","    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","    fps = cap.get(cv2.CAP_PROP_FPS)\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","    video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n","\n","    if not cap.isOpened():\n","        print(\"Error: Could not open video stream or file\")\n","        exit()\n","\n","    # --- ADDED: Setup CSV file for writing ---\n","    csv_headers = [\n","        'Timestamp (s)', 'Frame', 'Total People', 'Avg Age',\n","        'Men Count', 'Women Count', 'Excitement Index (Men)', 'Excitement Index (Women)',\n","        'Dominant Emotion', 'Dominant Emotion Count'\n","    ]\n","    with open(output_csv_path, 'w', newline='') as csv_file:\n","        csv_writer = csv.writer(csv_file)\n","        csv_writer.writerow(csv_headers) # Write the header row\n","\n","        cv2.namedWindow('Video Analysis', cv2.WINDOW_NORMAL)\n","        cv2.namedWindow('Crowd Statistics', cv2.WINDOW_NORMAL)\n","\n","        frame_count = 0\n","        process_interval = 15\n","        last_results = []\n","        start_time = time.time()\n","\n","        while cap.isOpened():\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","\n","            frame_count += 1\n","\n","            if frame_count % process_interval == 0:\n","                print(f\"Processing frame {frame_count} / {total_frames}...\")\n","                current_results = analyze_frame_attributes(frame)\n","                if current_results:\n","                    last_results = current_results\n","\n","                # --- ADDED: Log data to CSV only when analysis is run ---\n","                num_people = len(last_results)\n","                if num_people > 0:\n","                    total_age = sum(face['age'] for face in last_results)\n","                    avg_age = total_age / num_people\n","\n","                    gender_list = [face.get('dominant_gender', 'Unknown') for face in last_results]\n","                    emotion_list = [face.get('dominant_emotion', 'Unknown') for face in last_results]\n","                    gender_counts = Counter(gender_list)\n","                    emotion_counts = Counter(emotion_list)\n","\n","                    men_count = gender_counts.get('Man', 0)\n","                    women_count = gender_counts.get('Woman', 0)\n","\n","                    # Calculate gender-specific excitement\n","                    positive_emotions = ['happy', 'surprise']\n","                    positive_men = sum(1 for face in last_results if face.get('dominant_gender') == 'Man' and face.get('dominant_emotion') in positive_emotions)\n","                    positive_women = sum(1 for face in last_results if face.get('dominant_gender') == 'Woman' and face.get('dominant_emotion') in positive_emotions)\n","\n","                    excitement_index_men = positive_men / men_count if men_count > 0 else 0.0\n","                    excitement_index_women = positive_women / women_count if women_count > 0 else 0.0\n","\n","                    # Prepare data for CSV row\n","                    timestamp_sec = frame_count / fps\n","                    top_emotion_name = emotion_counts.most_common(1)[0][0] if emotion_counts else \"N/A\"\n","                    top_emotion_count = emotion_counts.most_common(1)[0][1] if emotion_counts else 0\n","\n","                    data_row = [\n","                        f\"{timestamp_sec:.2f}\", frame_count, num_people, f\"{avg_age:.1f}\",\n","                        men_count, women_count, f\"{excitement_index_men:.2f}\", f\"{excitement_index_women:.2f}\",\n","                        top_emotion_name, top_emotion_count\n","                    ]\n","                    csv_writer.writerow(data_row)\n","\n","            # --- Calculate Statistics (for display on every frame) ---\n","            num_people = len(last_results)\n","            total_age = 0\n","            gender_list = []\n","            emotion_list = []\n","\n","            # --- MODIFIED: Calculate two excitement indexes ---\n","            positive_emotions = ['happy', 'surprise']\n","            positive_men_count = 0\n","            positive_women_count = 0\n","\n","            for face_data in last_results:\n","                total_age += face_data['age']\n","                gender = face_data.get('dominant_gender', 'Unknown')\n","                emotion = face_data.get('dominant_emotion', 'Unknown')\n","                gender_list.append(gender)\n","                emotion_list.append(emotion)\n","\n","                if gender == 'Man' and emotion in positive_emotions:\n","                    positive_men_count += 1\n","                elif gender == 'Woman' and emotion in positive_emotions:\n","                    positive_women_count += 1\n","\n","            avg_age = total_age / num_people if num_people > 0 else 0\n","            gender_counts = Counter(gender_list)\n","            emotion_counts = Counter(emotion_list)\n","\n","            men_count = gender_counts.get('Man', 0)\n","            women_count = gender_counts.get('Woman', 0)\n","\n","            excitement_index_men = positive_men_count / men_count if men_count > 0 else 0.0\n","            excitement_index_women = positive_women_count / women_count if women_count > 0 else 0.0\n","\n","            # --- Create and Display Outputs ---\n","            # Pass the two new indexes to the display function\n","            stats_frame = create_stats_display(num_people, avg_age, gender_counts, emotion_counts, excitement_index_men, excitement_index_women)\n","            cv2.imshow('Crowd Statistics', stats_frame)\n","\n","            annotated_frame = draw_results_on_frame(frame, last_results)\n","            cv2.imshow('Video Analysis', annotated_frame)\n","\n","            video_writer.write(annotated_frame)\n","\n","            if cv2.waitKey(1) & 0xFF == ord('q'):\n","                break\n","\n","        end_time = time.time()\n","        print(\"\\nVideo processing complete.\")\n","        print(f\"Total time taken: {end_time - start_time:.2f} seconds.\")\n","        print(f\"Annotated video saved to: {output_video_path}\")\n","        print(f\"Analysis log saved to: {output_csv_path}\")\n","\n","        cap.release()\n","        video_writer.release()\n","        cv2.destroyAllWindows()"]}],"metadata":{"kernelspec":{"display_name":"Python 3.12 (deepface_env)","language":"python","name":"deepface_env"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.11"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}